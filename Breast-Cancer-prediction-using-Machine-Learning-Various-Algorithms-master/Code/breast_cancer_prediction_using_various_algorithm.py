# -*- coding: utf-8 -*-
"""Breast_Cancer_Prediction_Using_Various_Algorithm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q8m5-j3h3IU7CWGh5r76Dd3nJEagFNN0

# Breast Cancer Wisconsin (Diagnostic) Prediction
####Predict whether the cancer is benign or malignant

Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. n the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: "Robust Linear Programming Discrimination of Two Linearly Inseparable Sets", Optimization Methods and Software 1, 1992, 23-34].

This database is also available through the UW CS ftp server: ftp ftp.cs.wisc.edu cd math-prog/cpo-dataset/machine-learn/WDBC/

Also can be found on UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29

Attribute Information:

1) ID number 2) Diagnosis (M = malignant, B = benign) 3-32)

Ten real-valued features are computed for each cell nucleus:

a) radius (mean of distances from center to points on the perimeter) b) texture (standard deviation of gray-scale values) c) perimeter d) area e) smoothness (local variation in radius lengths) f) compactness (perimeter^2 / area - 1.0) g) concavity (severity of concave portions of the contour) h) concave points (number of concave portions of the contour) i) symmetry j) fractal dimension ("coastline approximation" - 1)

he mean, standard error and "worst" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.

All feature values are recoded with four significant digits.

Missing attribute values: none

Class distribution: 357 benign, 212 malignant
"""

# Commented out IPython magic to ensure Python compatibility.
# Import Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# %matplotlib inline

# Import Dataset
dataset = pd.read_csv("/content/drive/My Drive/Web Data/Breast_Cancer.csv")

dataset.head() # It'll show first 5 Dataset

dataset.drop(['id'],1,inplace=True) # We dropped id column because we doen't want anymore

dataset.head() # After Deleting Id Column

dataset['diagnosis']=dataset['diagnosis'].map({'M':1,'B':0})
''' In our Dataset There Are 2 Types Of Cancer Stages
1) M = malignant
2) B = benign
we can't input as string that's why i mapped into 0 and 1 form '''

dataset

dataset.columns

X = dataset[['texture_mean','perimeter_mean','smoothness_mean','compactness_mean','symmetry_mean','radius_se','area_se']]
# We take Indepent variables in 1 (X)
y = dataset['diagnosis']
# Y is depended variable

# We splitting dataset into 2 parts, Train and Test
from sklearn.model_selection import train_test_split
X_train , X_test , y_train , y_test = train_test_split(X,y , test_size=1/3 , random_state=0)

X.shape

y.shape

y

X_train # This is our Independent train dataset

y_train   # This is our Dependent train dataset

X_test # This is our Independent test dataset

y_test # This is our Dependent test dataset

"""# Simple Linear Regression 
###(Just for fun) Don't do this
"""

from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train,y_train)

y_pred = model.predict(X_test)

from sklearn import metrics as mt
mse = mt.mean_squared_error(y_test,y_pred)
RMSE = np.sqrt(mse)

RMSE

"""***''' There are 6 steps '''***


1. import Algorithm
2. Create object of algo
3. fit data into it 

4. Predict the value from algo

5. Create confusion matrix 
6. Fit data into confusion matrix find accuracy


# Congrachulations ! In just 6 steps You'll Make your Breast Cancer detection Machine Learning Models

# Logistic Regression
"""

from sklearn.linear_model import LogisticRegression # Step 1
modelLR = LogisticRegression(random_state=0) #Step 2
modelLR.fit(X_train,y_train)# Step 3

y_predLR = modelLR.predict(X_test) # Step 4

from sklearn.metrics import confusion_matrix # Step 5
cmlr = confusion_matrix(y_test,y_predLR) # step 6

cmlr

# 117+51 = 168 (Right Prediction)
# 17+5 = 22 (Wrong Prediction)
# Accuracy of Logistic Regression is : 88.42
# [(168*100) / 190] = 88.42

"""# Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
modelR = RandomForestClassifier(n_estimators = 10,criterion = 'entropy',random_state = 0)
modelR.fit(X_train,y_train)

y_predR = modelR.predict(X_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test,y_predR)

cm

# 119+58 = 177 (Right Prediction)
# 10+3 = 13 (Wrong Prediction)
# Random Forest Accuracy Is : 93.15        
# [(177*100)/190] = 93.15

"""# Support Vector Classifier"""

from sklearn.svm import  SVC
modelSVM = SVC(kernel = 'linear',random_state = 0)
modelSVM.fit(X_train,y_train)

y_predSVM = modelSVM.predict(X_test)

from sklearn.metrics import confusion_matrix
cmsvm = confusion_matrix(y_test,y_predSVM)

cmsvm

# 113+61 = 174 (Right Prediction)
# 7+9 = 16 (Wrong Prediction)
# SVM Accuracy Is : 91.57

"""# Decision Tree Classifier"""

from sklearn.tree import DecisionTreeClassifier
modelDTC = DecisionTreeClassifier(criterion = 'entropy')
modelDTC.fit(X_train,y_train)

y_predDTC = modelDTC.predict(X_test)

from sklearn.metrics import confusion_matrix
cmdtc = confusion_matrix(y_test,y_predDTC)

cmdtc

# 110+60 = 170 (Right Prediction)
# 12+8 = 20 (Wrong Prediction)
# SVM Accuracy Is : 89.47

"""# K-NN Classifier"""

from sklearn.neighbors import KNeighborsClassifier
modelKNN  = KNeighborsClassifier(n_neighbors=5,metric='minkowski',p=2)
modelKNN.fit(X_train,y_train)

y_predKNN = modelKNN.predict(X_test)

from sklearn.metrics import confusion_matrix
cmknn = confusion_matrix(y_test,y_predKNN)

cmknn

# 113+64 = 177 (Right prediction)
# 4+9 = 13 (Wrong Prediction)
# K-NN Accuracy is : 93.15

"""# Gaussian Naive bayes"""

from sklearn.naive_bayes import GaussianNB
modelGNB = GaussianNB()
modelGNB.fit(X_train,y_train)

y_predGNB = modelGNB.predict(X_test)

from sklearn.metrics import confusion_matrix
cmgnb = confusion_matrix(y_test,y_predGNB)

cmgnb

# 113+55 = 168 (Right Prediction)
# 13+9 = 22 (Wrong Prediction)
# Accuracy of Gaussian Naive Bayes is : 88.42

from IPython.display import HTML, display
import tabulate
table = [
         ["Random Forest",(93.15)],
         ["K-Nearest Neibour",(93.15)],
         ["Support Vector Classifier ",(91.57)],
         ["Decision Tree",(89.47)],
         ["Logistic Regression",(88.42)],
         ["Gaussian Naive Bayes",(88.42)] ]

"""# Observation"""

display(HTML(tabulate.tabulate(table, tablefmt='html')))